---
description: 
globs: 
alwaysApply: true
---
# Excel Automation Performance Rules
## Author: Ryan Combies
## Version: 2024-12-19

## Overview
This rule set provides performance optimization techniques for Excel automation using Microsoft.Office.Interop.Excel in IronPython/pyRevit environments. These techniques can dramatically improve data writing performance from minutes to seconds.

## Performance Optimization Strategies

### 1. Bulk Range Writing (Primary Strategy)
**Performance**: 10-100x faster than cell-by-cell operations
**Use Case**: Writing large datasets to Excel worksheets

```python
# FAST: Bulk range writing
def write_data_bulk(worksheet, data, start_row=2):
    """Write data using bulk range operations for maximum performance"""
    if not data:
        return
    
    total_rows = len(data)
    total_cols = len(data[0]) if data else 0
    
    # Prepare data as 2D array - convert all to strings for type safety
    bulk_data = []
    for row_data in data:
        processed_row = [str(value) if value is not None else "" for value in row_data]
        bulk_data.append(processed_row)
    
    # Define Excel range for all data
    start_cell = worksheet.Cells[start_row, 1]
    end_cell = worksheet.Cells[start_row + total_rows - 1, total_cols]
    data_range = worksheet.Range[start_cell, end_cell]
    
    # Convert to Excel format and write all at once
    excel_data = tuple(tuple(row) for row in bulk_data)
    data_range.Value2 = excel_data
```

### 2. Batch Range Writing (Fallback Strategy)
**Performance**: 5-20x faster than cell-by-cell operations
**Use Case**: When bulk writing fails or for memory management

```python
# GOOD: Batch range writing
def write_data_batches(worksheet, data, start_row=2, batch_size=500):
    """Write data in batches using range operations"""
    total_rows = len(data)
    
    for batch_start in range(0, total_rows, batch_size):
        batch_end = min(batch_start + batch_size, total_rows)
        batch_data = data[batch_start:batch_end]
        
        # Define range for this batch
        start_cell = worksheet.Cells[start_row + batch_start, 1]
        end_cell = worksheet.Cells[start_row + batch_end - 1, len(data[0]))
        batch_range = worksheet.Range[start_cell, end_cell]
        
        # Write batch data
        batch_excel_data = tuple(tuple(str(val) if val is not None else "" for val in row) for row in batch_data)
        batch_range.Value2 = batch_excel_data
```

### 3. Cell-by-Cell Writing (Last Resort)
**Performance**: Slowest method - avoid for large datasets
**Use Case**: Only when range operations fail

```python
# SLOW: Cell-by-cell writing (avoid for large datasets)
def write_data_cells(worksheet, data, start_row=2):
    """Write data cell-by-cell - use only as last resort"""
    for row_idx, row_data in enumerate(data):
        excel_row = start_row + row_idx
        for col_idx, value in enumerate(row_data, 1):
            worksheet.Cells[excel_row, col_idx].Value2 = str(value) if value is not None else ""
```

## 3-Tier Performance Strategy Implementation

### Complete Implementation Pattern
```python
def export_data_optimized(worksheet, data, start_row=2):
    """3-tier performance strategy for Excel data writing"""
    if not data:
        return
    
    total_rows = len(data)
    total_cols = len(data[0]) if data else 0
    
    # Prepare data once for all strategies
    bulk_data = []
    for row_data in data:
        processed_row = []
        for i in range(total_cols):
            if i < len(row_data):
                value = row_data[i]
                processed_row.append(str(value) if value is not None else "")
            else:
                processed_row.append("")
        bulk_data.append(processed_row)
    
    # Strategy 1: Try bulk writing (fastest)
    try:
        start_cell = worksheet.Cells[start_row, 1]
        end_cell = worksheet.Cells[start_row + total_rows - 1, total_cols]
        data_range = worksheet.Range[start_cell, end_cell]
        excel_data = tuple(tuple(row) for row in bulk_data)
        data_range.Value2 = excel_data
        print("Bulk data write completed successfully")
        return
    except Exception as e:
        print("Bulk write failed, falling back to batch writing: {0}".format(str(e)))
    
    # Strategy 2: Batch writing (fallback)
    batch_size = 500
    try:
        for batch_start in range(0, total_rows, batch_size):
            batch_end = min(batch_start + batch_size, total_rows)
            batch_data = bulk_data[batch_start:batch_end]
            
            start_cell = worksheet.Cells[start_row + batch_start, 1]
            end_cell = worksheet.Cells[start_row + batch_end - 1, total_cols]
            batch_range = worksheet.Range[start_cell, end_cell]
            batch_excel_data = tuple(tuple(row) for row in batch_data)
            batch_range.Value2 = batch_excel_data
            
            if total_rows > 1000:
                progress = int((batch_end / float(total_rows)) * 100)
                print("Writing data: {0}% complete".format(progress))
        return
    except Exception as e:
        print("Batch write failed, falling back to cell-by-cell: {0}".format(str(e)))
    
    # Strategy 3: Cell-by-cell (last resort)
    for row_idx, row_data in enumerate(bulk_data):
        try:
            excel_row = start_row + row_idx
            for col_idx, value in enumerate(row_data, 1):
                worksheet.Cells[excel_row, col_idx].Value2 = value
        except Exception as e:
            print("Error writing row {0}: {1}".format(excel_row, str(e)))
            continue
```

## File Locking Prevention

### File Lock Detection
```python
def is_file_locked(file_path):
    """Check if a file is locked/in use by another process"""
    try:
        if not os.path.exists(file_path):
            return False
        
        # Try to open in append mode (less intrusive)
        with open(file_path, 'a'):
            pass
        return False
    except IOError as e:
        error_msg = str(e).lower()
        if "permission denied" in error_msg or "being used by another process" in error_msg:
            return True
        return False
    except Exception:
        return False
```

### Excel File Closure
```python
def close_excel_file_if_open(file_path):
    """Attempt to close Excel file if open in any Excel instance"""
    try:
        excel_app = Excel.GetActiveObject("Excel.Application")
        for i in range(1, excel_app.Workbooks.Count + 1):
            wb = excel_app.Workbooks.Item(i)
            if wb.FullName.lower() == file_path.lower():
                wb.Close(SaveChanges=False)
                return True
        return False
    except:
        return False
```

## Error Handling Best Practices

### Graceful Degradation
- Always provide fallback strategies
- Isolate errors to prevent total failure
- Continue with partial data rather than failing completely

### User Communication
```python
# Good: Specific error messages with actionable guidance
if "being used by another process" in str(e).lower():
    forms.alert(
        "Cannot save Excel file - it may be open in another application.\n\n" +
        "File: {0}\n\n".format(os.path.basename(file_path)) +
        "Please close any Excel instances with this file and click 'Retry'.",
        title="File In Use During Save"
    )
```

## Performance Benchmarks

### Expected Performance Improvements
- **Small datasets** (< 1,000 rows): Near-instantaneous
- **Medium datasets** (1,000-10,000 rows): 2-5 seconds (vs 30-60 seconds)
- **Large datasets** (10,000+ rows): 10-30 seconds (vs several minutes)

### Memory Considerations
- **Batch size**: 500 rows recommended for balance of speed and memory usage
- **Data preparation**: Convert to strings once, reuse for all strategies
- **Type safety**: String conversion prevents Excel type errors

## Integration with Other Rules

### Compatible Rules
- `pyrevit-rules`: Excel export functionality in pyRevit scripts
- `revitpythonshell-rules`: Excel integration in RPS environments
- `dynamo-rules`: Excel output from Dynamo Python scripts

### Usage Context
Apply these rules when:
- Exporting large datasets to Excel
- Creating reports with hundreds/thousands of rows
- Building data analysis tools
- Implementing bulk data operations

## Implementation Checklist

- [ ] Use bulk range writing as primary strategy
- [ ] Implement batch writing as fallback
- [ ] Add cell-by-cell as last resort
- [ ] Include file lock detection
- [ ] Provide user-friendly error messages
- [ ] Add progress feedback for large datasets
- [ ] Convert data to strings for type safety
- [ ] Implement graceful error degradation

